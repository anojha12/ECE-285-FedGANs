\documentclass{article}
\usepackage[preprint,nonatbib]{neurips_2020}
\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}    
\usepackage{hyperref}       
\usepackage{url}            
\usepackage{booktabs}       
\usepackage{amsfonts}       
\usepackage{nicefrac}       
\usepackage{microtype}      
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{amsmath}
\title{Federated GANs: A Comparative Study of Privacy and Performance}

\author{%
  Karl Hernandez\\
  \texttt{kphernan@ucsd.edu}\\
  \and
  \textbf{Aneesh Ojha}\\
  \texttt{anojha@ucsd.edu}\\
  \and
  \textbf{Yingjieh Xia}\\
  \texttt{yix050@ucsd.edu}\\
}

\begin{document}

\maketitle
\vspace{-2em}
\section{Research Problem}

Modern Generative Adversarial Networks (GANs) require large and diverse datasets to generate high-quality, representative outputs. However, when data involves sensitive information—such as medical or financial records—centralized training poses serious privacy risks. [10]  Federated Learning (FL) enables collaborative model training across multiple data holders without sharing raw data, offering a promising privacy-preserving alternative. FL as it relates to GANs presents unique challenges in maintaining generation quality without centralized access. This project investigates whether federated GANs can deliver both strong privacy guarantees and competitive generative performance compared to centralized GANs.

\section{Importance of the Problem}
As machine learning becomes increasingly integrated into domains such as healthcare, finance, and personal data analytics, concerns about data privacy have become central. Traditional centralized training methods require aggregating data into a single location, which can expose sensitive information and violate legal or ethical guidelines.

FL offers a privacy-preserving alternative by keeping data decentralized across clients, training models locally and sharing only model updates. When combined with GANs, FL holds promise for generating realistic synthetic data while respecting privacy constraints. This is particularly valuable in domains where collecting large, diverse datasets is difficult due to privacy risks or data-sharing limitations.

However, the effectiveness of federated GANs (FedGANs) — in terms of both generation quality and privacy guarantees — remains an open question. Without centralized access to the full dataset, FedGANs can struggle to converge or represent the global data distribution accurately. Moreover, few works directly compare federated and non-federated GANs on realistic, privacy-sensitive tasks, especially with novel datasets reflective of healthcare or financial contexts.

Studying the trade-offs between federated and centralized GANs offers insights into:
\begin{itemize}[itemsep=0.5ex, topsep=0.5ex, parsep=0pt, partopsep=0pt]
  \item The privacy-performance tension in generative models
  \item Practical pathways to synthetic data generation for sensitive domains
  \item How privacy-preserving techniques affect the utility of generated data.
\end{itemize}

\section{Existing Work and Limitations}

Recent efforts have explored multiple strategies for embedding GANs[2] within decentralized training frameworks[3], yielding a diverse set of FedGAN architectures. MDGAN[1] presented the idea of a single, centralized generator broadcasting to clients, each hosting its own discriminator; this split allows the generator to benefit from varied local feedback while keeping raw data private. Building on that, FedGAN[4] adopts fully local generator–discriminator pairs and periodically synchronizes their parameters via a central server, striking a balance between model diversity and convergence stability.To support privacy guarantees, follow-up work has introduced formal protections. DPFedAVGGAN[5] integrates user-level differential privacy into the federated averaging step, adding calibrated noise to model updates so that sensitive training examples cannot be reverse‑engineered. Private FL‑GAN[6] further refines this by embedding privacy controls directly into the GAN training loop, ensuring that both generator and discriminator updates adhere to a prescribed privacy budget.

Despite these new architectures, several gaps remain. Most studies focus on image data, with limited application to tabular[9] or time-series domains. While DP is commonly used, empirical evaluations under privacy attacks remain sparse. Scalability in cross-device settings is hindered by communication overhead, and handling non-IID data remains challenging. Methods like FeGAN[7] and Universal Aggregation[8] attempt to address heterogeneity, but broader validation and benchmarking are still lacking.

\section{Proposed Solution}

To investigate the trade-off between generative performance and privacy in federated learning, we will train and evaluate two models: a centralized GAN and a federated GAN, using existing, well-established GAN architectures. Both models will be trained on the same novel dataset, chosen specifically for its relevance to privacy-sensitive applications.

Rather than developing new architectures, we aim to isolate the effect of the training paradigm (federated vs. centralized) by keeping the model design consistent. We will assess the models using standard generative metrics such as Frechet Inception Distance (FID) for image realism and Inception Score (IS) for class diversity.

In the federated setup, we will explore various aggregation strategies—including FedAvg, FedProx, and PreFed-GAN—to evaluate their impact on both performance and privacy. Additionally, we plan to evaluate each model’s robustness to privacy attacks (e.g., membership inference) to better understand the privacy-preserving capabilities of each training approach.

\section{Dataset}
The HAM10000 (“Human Against Machine with 10,000 dermatoscopic images”) dataset comprises a diverse collection of 10,015 dermatoscopic images of pigmented skin lesions—spanning seven diagnostic categories (melanoma, nevus, basal cell carcinoma, actinic keratosis, benign keratosis, dermatofibroma, and vascular lesions)—captured from multiple clinical centers worldwide. Each image is accompanied by expert-validated ground‐truth labels and extensive patient metadata (age, sex, lesion localization), making HAM10000 uniquely rich for both supervised and generative modeling. From a privacy standpoint, dermatoscopic images are highly sensitive: they can reveal personally identifiable features and carry significant risk if misused or leaked, thereby motivating strict data‐governance practices that naturally align with a federated learning paradigm. Despite its clear relevance, HAM10000 remains largely untapped in federated‐GAN research, where the majority of work continues to rely on generic object or face datasets (e.g., CIFAR, CelebA) that lack medical complexity and realistic non-IID client distributions. In this study, we leverage HAM10000 to perform a controlled empirical comparison of federated and non-federated (centralized) GANs under identical experimental conditions—partitioning images by acquisition site to simulate real-world clinical heterogeneity—thereby demonstrating how privacy‐preserving federated GANs can generate high-fidelity synthetic dermatoscopic images without ever centralizing raw patient data.

\section{Method}

\subsection{GAN}
The original GAN[2] consists of two networks: Generator and Discriminator, and both G and D could be a non-linear mapping function, such as a multi-layer perceptron. Generator G captures the data distribution, while discriminator D estimates the probability that a sample came from the training data rather than G.
The update function for $D$ and $G$ is:
\[
\min_{G} \max_{D} V(D,G) = \mathbb{E}_{x \sim p_{\text{data}}(x)} [\log D(x)] + \mathbb{E}_{z \sim p_{z}(z)} [\log (1-D(G(z)))]
\]


\subsection{Federated GAN}

\subsubsection{Architecture - Universal Aggregation GAN (UA GAN)}
Universal Aggregation GAN[11] proposed a model which has one central generator G, and multiple local and privately hosted discriminators. Each local discriminator is only trained on its local data and help the generator to update. Each private data may contain both common distributions and different distributions (non-iid), which might be a lot. The challenge for the generator is to learn the distribution that represent all the private data given separate discriminators.

\subsubsection{Goal}
Suppose there are $K$ entities holding K private datasets $D_1, ..., D_K$, with size $n_1, ..., n_K$. The total data size $n = \sum_{j=1}^{K} n_j$.
Learn a target mixture distribution:
\[
p(x) = \sum_{j=1}^{K} \pi_j p_j(x) 
\]
where $p_j(x)$ is the distribution from the j-th local dataset $D_j$, and the weight $\pi_j$ is the fraction of dataset $D_j$: $\pi_j = n_j / n$. Note that different $p_i(x)$ and $p_j(x)$ might be non-iid.

\subsubsection{Universal Aggregation}
To aggregate the information from the local discriminators, an unbiased centralized discriminator is proposed, with its odds value approximates that of the mixture of local discriminators. By aggregating gradient from local discriminators based on the odds value of the central value, the model can learn the desired mixture of local distributions.

\subsubsection{Local Discriminator}
Each local discriminator $D_j$ has access to its own data from distribution $p_j(x)$ aiming to minimize the cross entropy loss:
\[
\max_{D_j} V(D_j,G) = \mathbb{E}_{x \sim p_{j}(x)} [\log D_j(x)] + \mathbb{E}_{z \sim p_{z}(z)} [\log (1-D_j(G(z)))]
\]


\subsubsection{Centralized Discriminator and Generator}
The main idea of universal aggregation is to simulate a centralized discriminator $D_{ua}$ which behaves like the mixture of all local discriminators in terms of odds value, then use this $D_{ua}$ to train the centralized $G$, like a classical GAN.

Definition of odds value: Given a probability $\phi \in [0, 1)$, its odds value is $\Phi(\phi) = \phi / (1-\phi)$, where $\phi = \Phi(\phi) / (1+\Phi(\phi))$.

We can then get the odds value of the centralized discriminator by summing up the weighted odds value of each local $D_j$:
\[
\Phi(D_{ua}(x)) = \sum_{j=1}^K \pi_j \Phi(D_j(x))
\]

And we can get the the probability of $D_{ua}(x)$:
\[
D_{ua}(x) = \frac{\Phi(D_{ua}(x))}{1+\Phi(D_{ua}(x))}
\]

Once we get $D_{ua}(x)$, we can train the centralized generator $G$ by minimizing the loss:
\[
\min_{G} V(D_{ua},G) = \mathbb{E}_{x \sim p(x)} [\log D_{ua}(x)] + \mathbb{E}_{z \sim p_{z}(z)} [\log (1-D_{ua}(G(z)))]
\]

The reason to use odds value instead of average value of every local discriminator is that from the original GAN paper, it has been shown that a optimal $D^*$ given a data distribution $p_{\text{data}}(x)$ and a fixed generator distribution $p_{g}(x)$ satisfies $D^*{(x)} = p_{\text{data}}(x) / (p_{\text{data}}(x) + p_{g}(x))$. Thus, the centralized $D_{ua}$ can be optimal only when using odds value, and only when all the local $D_j$ are optimal.
Given a optimal $D_{ua}$, we can theoretically get a optimal generator $g^*$ using w.r.t the Henson Shannon divergence loss:
\[
g^* = \text{argmin}_g L(g) = E_{x \sim p(x)} [log D_{ua}(x)] + E_{x \sim g(x)} [log 1-D_{ua}(x)]
\]

where $g^*$ equals to the true distribution.


\subsubsection{Universal Aggregation for Conditional GAN}
A conditional GAN[12] learns the joint distribution of $p(x, y)$, where x represents an image or a vectorized data, and y is an auxiliary variable to control the mode of generated data (e.g. the class label of an image/data). The target mixture distribution then becomes:
\[
p(x) = \sum_{j} \pi_j \omega_j(y) p_j(x,y)
\]
where $\pi_j = n_j / n$, and $\omega_j(y)$ is the proportion of class $y$ data within the $j$-th local dataset $D_j$.
To adapt UA to use conditional GAN, the odds value of $D_{ua}$ needs to be changed to:
\[
\Phi(D_{ua}(x|y)) = \sum_{j=1}^K \pi_j \omega_j(y) \Phi(D_j(x|y))
\]
The $D_{ua}(x|y)$, and the update of $G$ and $D_j$ need to be adjusted accordingly.


\section{Evaluation and Metrics}
In our evaluation framework, we employ a \textbf{membership-inference attack (MIA)} to quantify the extent to which a trained generator $G$ inadvertently memorizes and leaks information about its training set $\mathcal{D}_{\mathrm{train}}$.At its core, the membership-inference metric evaluates a model's tendency to ``memorize'' individual training samples by measuring how distinguishable those samples are from unseen data. When an adversary achieves a high true-positive rate (TPR) at the same false-positive rate (FPR) (or, equivalently, a large $\lvert\mathrm{TPR} - \mathrm{FPR}\rvert$ advantage), it implies that the generator's outputs for training examples lie in regions of the data manifold that are noticeably different—either in likelihood under the discriminator or in some other feature space—than outputs for non-training examples. In other words, memorization creates ``fingerprints'' that the adversary can exploit. Conversely, if the model generalizes well and does not overfit, its behavior on training and non-training samples becomes statistically indistinguishable, driving the membership advantage toward zero ($\mathrm{Adv}_{\mathcal{A}}\approx 0$) and the ROC AUC toward 0.5—equivalent to a random guess. Thus, lower membership-inference performance directly corresponds to stronger empirical privacy protection.\\
Concretely, an adversary $\mathcal{A}$ is given a sample $x$ (either drawn from $\mathcal{D}_{\mathrm{train}}$ or from an unseen hold-out set $\mathcal{D}_{\mathrm{test}}$) and produces a binary decision
\begin{align}
m = \mathcal{A}(x) \;=\; \begin{cases} 
1, & \text{if } \mathcal{A}\text{ judges }x\in \mathcal{D}_{\mathrm{train}},\\ 
0, & \text{otherwise}. 
\end{cases}
\end{align}

We measure privacy leakage in terms of the adversary's true-positive rate (TPR) and false-positive rate (FPR):
\begin{align}
\mathrm{TPR} \;=\; \Pr\bigl(\mathcal{A}(x)=1 \mid x\in\mathcal{D}_{\mathrm{train}}\bigr), \quad \mathrm{FPR} \;=\; \Pr\bigl(\mathcal{A}(x)=1 \mid x\in\mathcal{D}_{\mathrm{test}}\bigr).
\end{align}

From these we derive the \textbf{membership advantage},
\begin{align}
\mathrm{Adv}_{\mathcal{A}}(G) \;=\; \bigl|\mathrm{TPR} - \mathrm{FPR}\bigr|,
\end{align}
which captures the adversary's maximal distinguishing power. We further compute the overall attack accuracy
\begin{align}
\mathrm{Acc}_{\mathcal{A}} \;=\; \frac{\mathrm{TP} + \mathrm{TN}}{\mathrm{TP} + \mathrm{TN} + \mathrm{FP} + \mathrm{FN}}
\end{align}
and the area under the ROC curve (AUC), denoted $\mathrm{AUC}_{\mathcal{A}}$. A higher $\mathrm{Adv}$, $\mathrm{Acc}$, or $\mathrm{AUC}$ indicates stronger privacy leakage, since the adversary can more reliably infer whether any given sample was used during training.

To compare FedGAN against a conventionally trained Non-Federated GAN, we apply the identical MIA pipeline—same adversary architecture, training-set sizes, and decision thresholds—to each generator. Because FedGAN's updates are computed and aggregated across multiple clients without ever centralizing raw data, it is hypothesized to memorize less about any individual client's samples. Accordingly, we expect
\begin{align}
\mathrm{Adv}_{\mathcal{A}}\bigl(G_{\mathrm{Fed}}\bigr)\;<\;\mathrm{Adv}_{\mathcal{A}}\bigl(G_{\mathrm{Normal}}\bigr), \quad \mathrm{AUC}_{\mathcal{A}}\bigl(G_{\mathrm{Fed}}\bigr)\;<\;\mathrm{AUC}_{\mathcal{A}}\bigl(G_{\mathrm{Normal}}\bigr),
\end{align}
with corresponding decreases in $\mathrm{TPR}-\mathrm{FPR}$ and peak ROC performance. By reporting these metrics side-by-side—together with confidence intervals over multiple random seeds—we will obtain a principled, quantitative assessment of how federated training mitigates membership leakage, thereby providing strong empirical evidence that FedGAN offers enhanced privacy guarantees relative to Non-Federated GAN.

\section{Novelty and Significance}
Our contributions include:
\begin{itemize}
  \item Use of an existing, underutilized dataset for evaluating generative models in privacy-sensitive settings.
  \item Direct empirical comparison of federated and non-federated (centralized) GANs under identical experimental conditions.
  \item Focused assessment of privacy-preserving qualities of both model types, including vulnerability to inference attacks.
  \item Quantitative evaluation of trade-offs between privacy and generative performance.
\end{itemize}
This study will provide practical insights into how well federated approaches preserve privacy and it's affect on model performance which can inform appropriate deployment choices for different applications.

\section{Tentative Timeline}

\begin{itemize}
    \item Weeks 1–2: Review related work and refine understanding of architectures
    \item Week 3: Develop pipeline and establish FL and non-FL baselines
    \item Weeks 4–5: Implement Federated GAN and train Centralized GAN
    \item Week 6: Conduct privacy attacks and analyze results
    \item Week 7: Write and finalize documentation and report
\end{itemize}


\section*{References}

{
\small
[1] McMahan, B., Moore, E., Ramage, D., \& Hampson, S.\ (2017) Communication-efficient learning of deep networks from decentralized data. In AISTATS 2017, {\it Proceedings of the 20th International Conference on Artificial Intelligence and Statistics}, Vol.\ 54.

[2] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., \& Bengio, Y.\ (2014) Generative adversarial nets. In Z.\ Ghahramani, M.\ Welling, C.\ Cortes, N.D.\ Lawrence, \& K.Q.\ Weinberger (eds.), {\it Advances in Neural Information Processing Systems 27}, pp.\ 2672--2680. Cambridge, MA: MIT Press.

[3] Hardy, Q., Le Merrer, E., \& Trédan, G.\ (2019) MD-GAN: Multi-discriminator generative adversarial networks for distributed datasets. {\it arXiv preprint arXiv:1911.03860}.

[4] Rasouli, A., Hashemi, S.A., Rouhani, B., Riazi, M.S., \& Koushanfar, F.\ (2020) FedGAN: Federated generative adversarial networks for distributed data. In {\it Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops}, pp.\ 1--10.

[5] Augenstein, S., McMahan, H.B., Ramage, D., \& Ramaswamy, K.\ (2019) Differentially private federated learning for text classification. {\it arXiv preprint arXiv:2004.11791}.

[6] Xin, Y., Liu, D., Ma, J., Wang, W., \& Tao, D.\ (2020) Private federated generative adversarial networks. In {\it Proceedings of the AAAI Conference on Artificial Intelligence}, Vol.\ 34, No.\ 4, pp.\ 7163--7170.

[7] Guerraoui, R., Rouault, S., Tazi, I., \& Vuilleumier, P.\ (2020) Fegan: Federated generative model learning. {\it arXiv preprint arXiv:2006.07219}.

[8] Zhang, H., Wu, X., Liu, J., Chang, S., \& Han, S.\ (2021) A universal aggregation framework for federated learning. In M.\ Ranzato, A.\ Beygelzimer, Y.\ Dauphin, P.S.\ Liang, \& J.\ Wortman Vaughan (eds.), {\it Advances in Neural Information Processing Systems 34}, pp.\ 17390--17401. Red Hook, NY: Curran Associates.

[9] Maliakel, T., Rajendran, J., Lalitha, A., \& Krishnan, R.\ (2024) FLIGAN: Federated Learning of Imbalanced Tabular Data Using GANs. {\it arXiv preprint arXiv:2403.08744}.

[10] A. Golda et al., (2024) Privacy and Security Concerns in Generative AI: A Comprehensive Survey,". In \it{IEEE Access, vol. 12, pp. 48126-48144}

[11] Zhang, Y., Qu, H., Chang, Q., Liu, H., Metaxas, D., & Chen, C. (2021). Training federated GANs with theoretical guarantees: A universal aggregation approach. arXiv preprint arXiv:2102.04655.

[12] Mirza, M., & Osindero, S. (2014). Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784.
}




\end{document}